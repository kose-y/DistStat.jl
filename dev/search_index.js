var documenterSearchIndex = {"docs":
[{"location":"#DistStat.jl:-Towards-Unified-Programming-for-High-performance-Statistical-Computing-Environments-in-Julia","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"","category":"section"},{"location":"#Introduction","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"Introduction","text":"","category":"section"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"DistStat.jl implements a distributed array data structure on both distributed CPU and GPU environments and also provides an easy-to-use interface to the structure in the programming language Julia. A user can switch between the underlying array implementations for CPU cores or GPUs only with minor configuration changes. Furthermore, DistStat.jl can generalize to any environment on which the message passing interface (MPI) is supported. This package leverages on the built-in support for multiple dispatch and vectorization semantics of Julia, resulting in easy-to-use syntax for elementwise operations and distributed linear algebra.","category":"page"},{"location":"#Software-Interface","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"Software Interface","text":"","category":"section"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"DistStat.jl implements a distributed MPI-based array data structure MPIArray as the core data structure for implementations of AbstractArrays. It uses MPI.jl as a backend. It has been tested for basic Arrays and CuArrays from CUDA.jl. The standard vectorized \"dot\" operations can be used for convenient element-by-element operations as well as broadcasting operations on MPIArrays. Furthermore, simple distributed matrix operations for MPIMatrix, or two-dimensional MPIArrays, are also implemented. Reduction and accumulation operations are supported for MPIArrays of any dimensions.   The package can be loaded by:","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"using DistStat","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"If GPUs are available, one that is to be used is automatically selected in a round-robin fashion upon loading the package. The rank, or the \"ID\" of a process, and the size, or the total number of the processes, can be accessed by:","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"DistStat.Rank()\nDistStat.Size()","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"Ranks are indexed 0-based, following the MPI standard.","category":"page"},{"location":"#Data-Structure-for-Distributed-MPI-Array","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"Data Structure for Distributed MPI Array","text":"","category":"section"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"In DistStat.jl, a distributed array data type MPIArray{T,N,AT} is defined. Here, parameter T is the type of each element of an array, e.g., Float64 or Float32. Parameter N is the dimension of the array, 1 for vector and 2 for matrix. Parameter AT is the implementation of AbstractArray used for base operations: Array for CPU array, and CuArray for the arrays on Nvidia GPUs (requires CUDA.jl). If there are multiple CUDA devices, a device is assigned to a process automatically by teh rank of the process modulo the size. This assignment scheme extends to the setting in which there are multiple GPU devices in multiple CPU nodes. The type MPIArray{T,N,AT} is a subtype of AbstractArray{T,N}. In MPIArray{T,N,AT, each rank holds a contiguous block of the full data in AT{T,N} split by the N-th dimension, or the last dimension of an MPIArray.","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"In the special case of a two-dimenaional array, aliased by MPIMatrix{T,AT}, the data is column-major ordered and column-split. The transpose of this matrix has type of  Transpose{T,MPIMatrix{T,AT}} which is row-major ordered, row-split. There also is an alias for one-dimensional array MPIArray{T,1,A}, which is MPIVector{T,A}.","category":"page"},{"location":"#Creation","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"Creation","text":"","category":"section"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"The syntax MPIArray{T,N,A}(undef, m, ...) creates an ininitialized MPIArray. For example,","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"a = MPIArray{Float64, 2, Array}(undef, 3, 4)","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"creates an uninitialized 3 times 4 distributed array based on local Arrays of double precision floating-point numbers. The size of this array, the type of each element, and number of dimensions can be accessed using the usual functions in Julia:","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"size(a)\neltype(a)\nndims(a)","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"Local data held by each process can be accessed by appending .localarray to the name of the array, e.g., ","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"a.localarray","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"Matrices are split as evenly as possible. For example, if the number of processes is 4 and the size(a) == (3, 7), processes of rank 0 through 2 hold the local data of size (3, 2) and the rank-3 process holds the local data of size (3, 1).","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"An MPIArray can also be created by distributing an array in a single process. For example, in the following code: ","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"if DistStat.Rank() == 0\n    dat = [1, 2, 3, 4]\nelse\n    dat = Array{Int64}(undef, 0)\nend\nd = distribute(dat)","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"the data is defined in rank 0 process, and the other processes have an empty instance of Array{Int64}. Using the function distribute, the MPIArray{Int64, 1, Array} of the data [1, 2, 3, 4], equally distributed over four processes, is created.","category":"page"},{"location":"#Filling-an-array","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"Filling an array","text":"","category":"section"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"An MPIArray a can be filled with a  number x using the usual syntax of the function fill!(a, x). For example, a can be filled with zero:","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"fill!(a, 0)","category":"page"},{"location":"#Random-number-generation","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"Random number generation","text":"","category":"section"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"An array can also be filled with random values, extending Random.rand!() for the standard uniform distribution and Random.randn()! for the standard normal distribution. The following code fills a with uniform(0, 1) random numbers:","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"using Random\nrand!(a)","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"In cases such as unit testing, generating identical data for any configuration is important. For this purpose, the following interface is defined:","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"function rand!(a::MPIArray{T,N,A}; seed=nothing, common_init=false, root=0) where {T,N,A}","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"If the keyword argument common_init=true is set, the data are generated from the process with rank root. The seed can also be configured. If commonn_init==false and seed==k, the seed for each process is defined by k plus the rank.","category":"page"},{"location":"#\"Dot\"-syntax-and-vectorization","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"\"Dot\" syntax and vectorization","text":"","category":"section"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"The \"dot\" broadcasting feature of DistStat.jl follows the standard Julia syntax. This syntax provides a convenient way to operate on both multi-node clusters and multi-GPU workstations with the same code. For example, the soft-thresholding operator, which commonly appears in sparse regression can be defined in the element level:","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"function soft_threshold(x::T, λ::T) ::T where T <: AbstractFloat\n    x > λ && return (x - λ)\n    x < -λ && return (x + λ)\n    return zero(T)\nend","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"This function can be applied to each element of an MPIArray using the dot broadcasting, as follows. When the dot operation is used for an MPIArray{T,N,AT}, it is naturally passed to inner array implementation AT. Consider the following arrays filled with random numbers from the standard normal distribution:","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"a = MPIArray{Float64, 2, Array}(undef, 2, 4) |> randn!\nb = MPIArray{Float64, 2, Array}(undef, 2, 4) |> randn!","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"The function soft_threshold() is applied elementwisely as the following:","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"a .= soft_threshold.(a .+ 2 .* b, 0.5)","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"The three dot operations, .=, .+, and .*, are fused into a single loop (in CPU) or a single kernel (in GPU) internally. ","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"A singleton non-last dimension is treated as if the array is repeated along that dimension, just like Array operations.","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"c = MPIArray{Float64, 2, Array}(undef, 1, 4) |> rand!\na .= soft_threshold.(a .+ 2 .* c, 0.5)","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"works as if c were a 2 times 4 array, with its content repeated twice.","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"It is a little bit subtle with the last dimension, as the MPIArray{T,N,AT}s are split along that dimension. It works if the broadcast array has the type AT and holds the same data across the processes. For example,","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"d = Array{Float64}(undef, 2, 1); fill!(d, -0.1)\na .= soft_threshold.(a .+ 2 .* d, 0.5)","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"As with any dot operations in Julia, the operatons for DistStat.jl are convenient but usually not the fastest option, Its implementations can be further optimized by specializeing in specific array types.","category":"page"},{"location":"#Reduction-opeartions-and-accumulation-operations","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"Reduction opeartions and accumulation operations","text":"","category":"section"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"Reduction operations, such as sum(), prod(), maximum(), minimum(), and accumulations such as cumsum(), cumsum!(), cumprod(), cumprod!() are implemented just like their base counterparts, computing cumulative sums and products. Example usages of sum() and sum!() are:","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"sum(a)\nsum(abs2, a) # sum of squared absolute value\nsum(a, dims=1) # column sums\nsum(a, dims=2)\nsum(a, dims=(1,2)) # returns 1x1 MPIArray\nsum!(c, a) # columnwise sum \nsum!(d, a) # rowwise sum","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"The first line computes the elementwise sum of a. The second line computes the sum of squared absolute values (abs2() is the method that computes the squared absolute values). The third and fourth lines compute the column sums and row sums, respectively. Similar to the dot operations, the third line reduces along the distributed dimensions, and returns a broadcast local Array. The fifth line returns the sum of all elements, but the data type is a 1 times 1 MPIArray. The syntax sum!(p, q) selects which dimension to reduce based on the shape of p, the first argument. The sixth line computes the columnwise sum and saves it to c, because c is a 1 times 4 MPIArray. The seventh line computes rowwise sum, because d is a 2 times 1 local Array.  ","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"Given below are examples for cumsum() and cumsum!():","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"# Accumulations\ncumsum(a; dims=1) # columnwise accumulation\ncumsum(a; dims=2) # rowwise accumulation\ncumsum!(b, a; dims=1) # columnwise, result saved in `b`\ncumsum!(b, a; dims=2)","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"The first line computes the columnwise cumulative sum, and the second line computes the rowwise cumulative sum. So do the third and fourth lines, but save the results in b, which has the same size as a. ","category":"page"},{"location":"#Distributed-Linear-Algebra","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"Distributed Linear Algebra","text":"","category":"section"},{"location":"#Dot-product","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"Dot product","text":"","category":"section"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"The method LinearAlgebra.dot() for MPIArrays is defined just like the base LinearAlgebra.dot(), which sums all the elements after an elementwise multiplication of the two argument arrays: ","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"using LinearAlgebra\ndot(a, b)","category":"page"},{"location":"#Operations-on-the-diagonals","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"Operations on the diagonals","text":"","category":"section"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"The \"getter\" method for the diagonal, diag!(d, a), and the \"setter\" method for the diagonal, fill_diag!(), are also available. The former obtains the main diagonal of the MPIMatrix a and is stored in d. If d is an MPIMatrix with a single row, the result is obtained in a distributed form. On the other hand, if d is a local AbstractArray, all elements of the main diagonal is copied to all processes as a broadcast AbstractArray:","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"M = MPIMatrix{Float64, Array}(undef, 4, 4) |> rand!\nv_dist = MPIMatrix{Float64, Array}(undef, 1, 4)\nv = Array{Float64}(undef, 4)\ndiag!(v_dist, M)\ndiag!(v, M)","category":"page"},{"location":"#Matrix-multiplication","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"Matrix multiplication","text":"","category":"section"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"The method LinearAlgebra.mul!(C, A, B) is implemented for MPIMatrix, in which the multiplication of A and B is stored in C. Matrix multiplications for 17 different combinations of types for A, B, and C, including matrix-vector multiplications are included in the package. It is worth noting that transpose of an MPIMatrix is a row-major ordered, row-split matrix. While the base syntax of mul!(C, A, B) is always available, any temporary memory to save intermediate results can also be provided as a keyword argument in order to avoid repetitive allocations in iterative algorithms, as in mul!(C, A, B; tmp=Array(undef, 3, 4). The user should determine which shape of C minimizes communication and suits better for their application. MPIColVector{T, AT} is defined as Union{MPIVector{T,AT}, Transpose{T, MPIMatrix{T,AT}}} to include transposed MPIMatrix with a single row. The 17 possible combinations of arguments available are listed below:","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"LinearAlgebra.mul!(C::MPIMatrix{T,AT}, A::MPIMatrix{T,AT}, B::Transpose{T, MPIMatrix{T,AT}};\n                            tmp::AbstractArray{T,2}=AT{T}(undef, size(A,1), size(B,2))) where {T,AT}\nLinearAlgebra.mul!(C::Transpose{T,MPIMatrix{T,AT}}, A::MPIMatrix{T,AT},B::Transpose{T,MPIMatrix{T,AT}};\n                            tmp::AbstractArray{T,2}=AT{T}(undef, size(B,2), size(A,1))) where {T,AT}\nLinearAlgebra.mul!(C::MPIMatrix{T,AT}, A::MPIMatrix{T,AT}, B::MPIMatrix{T,AT};\n                            tmp::AbstractArray{T,2}=AT{T}(undef, size(A,1), size(A,2))) where {T,AT}\nLinearAlgebra.mul!(C::Transpose{T,MPIMatrix{T,AT}}, A::Transpose{T,MPIMatrix{T,AT}}, B::Transpose{T,MPIMatrix{T,AT}};\n                            tmp::AbstractArray{T,2}=AT{T}(undef, size(B,2), size(B,1))) where {T,AT}\nLinearAlgebra.mul!(C::AbstractMatrix{T}, A::MPIMatrix{T,AT}, B::Transpose{T,MPIMatrix{T,AT}}) where {T,AT}\nLinearAlgebra.mul!(C::MPIMatrix{T,AT}, A::Transpose{T,MPIMatrix{T,AT}}, B::MPIMatrix{T,AT};\n                            tmp::AbstractArray{T,2}=AT{T}(undef,size(A,2),size(A,1))) where {T,AT}\nLinearAlgebra.mul!(C::Transpose{T,MPIMatrix{T,AT}}, A::Transpose{T,MPIMatrix{T,AT}}, B::MPIMatrix{T,AT};\n                            tmp::AbstractArray{T,2}=AT{T}(undef,size(B,1), size(B,2))) where {T,AT}\nLinearAlgebra.mul!(C::MPIMatrix{T,AT}, A::Union{AbstractMatrix{T}}, B::MPIMatrix{T,AT}) where {T,AT}\nLinearAlgebra.mul!(C::MPIMatrix{T,AT}, A::Transpose{T,ATT} where ATT <: AbstractMatrix{T}, B::MPIMatrix{T,AT}) where {T,AT}\nLinearAlgebra.mul!(C::Transpose{T,MPIMatrix{T,AT}}, A::Transpose{T,MPIMatrix{T,AT}},\n                            B::Transpose{T, ATT} where ATT <: AbstractMatrix{T}) where {T,AT}\nLinearAlgebra.mul!(C::Transpose{T,MPIMatrix{T,AT}}, A::Transpose{T,MPIMatrix{T,AT}}, B::AbstractMatrix{T}) where {T,AT}\nLinearAlgebra.mul!(C::AbstractVector{T}, A::MPIMatrix{T,AT}, B::AbstractVector{T}) where {T,AT}\nLinearAlgebra.mul!(C::AbstractVector{T}, A::Transpose{T, MPIMatrix{T,AT}}, B::AbstractVector{T}) where {T,AT}\nconst MPIColVector{T,AT} = Union{MPIVector{T,AT},Transpose{T,MPIMatrix{T,AT}}}\nLinearAlgebra.mul!(C::AbstractVector{T}, A::MPIMatrix{T,AT}, B::MPIColVector{T,AT}) where {T,AT}\nLinearAlgebra.mul!(C::MPIColVector{T,AT}, A::Transpose{T,MPIMatrix{T,AT}}, B::AbstractVector{T}) where {T,AT} \nLinearAlgebra.mul!(C::MPIColVector{T,AT}, A::MPIMatrix{T,AT}, B::MPIColVector{T,AT};\n                            tmp::AbstractArray{T}=AT{T}(undef, size(C, 1))) where {T,AT}\nLinearAlgebra.mul!(C::MPIColVector{T,AT}, A::Transpose{T,MPIMatrix{T,AT}},B::MPIColVector{T,AT};\n                            tmp::AbstractArray{T}=AT{T}(undef, size(B,1))) where {T,AT}","category":"page"},{"location":"#Operator-norms","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"Operator norms","text":"","category":"section"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"The method opnorm() either evaluates (ell_1 and ell_infty) or approximates (ell_2)  matrix operator norms, defined for a matrix A in mathbbR^m times n as A = supAx x in mathbbR^n text with  x = 1 for each respective vector norm. ","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"opnorm(a, 1)\nopnorm(a, 2)\nopnorm(a, Inf)","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"The ell_2-norm estimation implements the power iteration, and can be further configured for convergence and number of iterations. There also is an implementation based on the inequality A_2 le A_1 A_infty (method=\"quick\"), which overestimates the ell_2-norm.","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"opnorm(a, 2; method=\"power\", tol=1e-6, maxiter=1000, seed=95376)","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"opnorm(a, 2; method=\"quick\")","category":"page"},{"location":"#Simplified-MPI-Interfaces","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"Simplified MPI Interfaces","text":"","category":"section"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"DistStat.jl also provides a simplified version of MPI primitives. These primitives allow omission of the constant MPI.COMM_WORLD, tag for communication, and the root if it is zero. They are modeled after the convention of the distributed subpackage of PyTorch, but with richer interfaces including Allgatherv!(), Gatherv!(), and Scatterv!(). These primitives are not only exposed to the user, but also extensively used in the linear algebra and array operation routines explained above. Arrays can be used as arguments fo rthe following functions, and CuArrays can be used if CUDA-aware MPI implementation such as OpenMPI is available on the system. The following list shows the signatures of simplified MPI methods. See documentations of MPI.jl for more information.","category":"page"},{"location":"","page":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","title":"DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia","text":"Barrier()\nBcast!(arr::AbstractArray; root::Integer=0)\nSend(arr::AbstractArray, dest::Integer; tag::Integer=0)\nRecv!(arr::AbstractArray, src::Integer; tag::Integer=0)\nIsend(arr::AbstractArray, dest::Integer; tag::Integer=0)\nIrecv!(arr::AbstractArray, src::Integer; tag::Integer=0)\nReduce!(sendarr::AbstractArray, recvarr::AbstractArray; \n    op=MPI.SUM, root::Integer=0)\nAllreduce!(arr::AbstractArray; op=MPI.SUM)\nAllgather!(sendarr::AbstractArray, recvarr::AbstractArray)\nAllgather!(sendarr::AbstractArray, recvarr::AbstractArray, \n    count::Integer)\nAllgatherv!(sendarr::AbstractArray, recvarr::AbstractArray, \n    counts::Vector{<:Integer})\nScatter!(sendarr::AbstractArray, recvarr::AbstractArray; \n    root::Integer=0)\nScatterv!(sendarr::AbstractArray, recvarr::AbstractArray, \n    counts::Vector{<:Integer}; root::Integer=0)\nGather!(sendarr::AbstractArray, recvarr::AbstractArray; \n    root::Integer=0)\nGatherv!(sendarr::AbstractArray, recvarr::AbstractArray, \n    counts::Vector{<:Integer}; root::Integer=0)","category":"page"}]
}
