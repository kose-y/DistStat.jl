<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia · DistStat.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">DistStat.jl</span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#Software-Interface"><span>Software Interface</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kose-y/DistStat.jl/blob/master/docs/src/index.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="DistStat.jl:-Towards-Unified-Programming-for-High-performance-Statistical-Computing-Environments-in-Julia"><a class="docs-heading-anchor" href="#DistStat.jl:-Towards-Unified-Programming-for-High-performance-Statistical-Computing-Environments-in-Julia">DistStat.jl: Towards Unified Programming for High-performance Statistical Computing Environments in Julia</a><a id="DistStat.jl:-Towards-Unified-Programming-for-High-performance-Statistical-Computing-Environments-in-Julia-1"></a><a class="docs-heading-anchor-permalink" href="#DistStat.jl:-Towards-Unified-Programming-for-High-performance-Statistical-Computing-Environments-in-Julia" title="Permalink"></a></h1><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p><code>DistStat.jl</code> implements a distributed array data structure on both distributed CPU and GPU environments and also provides an easy-to-use interface to the structure in the programming language Julia. A user can switch between the underlying array implementations for CPU cores or GPUs only with minor configuration changes. Furthermore, <code>DistStat.jl</code> can generalize to any environment on which the message passing interface (<code>MPI</code>) is supported. This package leverages on the built-in support for multiple dispatch and vectorization semantics of Julia, resulting in easy-to-use syntax for elementwise operations and distributed linear algebra.</p><h2 id="Software-Interface"><a class="docs-heading-anchor" href="#Software-Interface">Software Interface</a><a id="Software-Interface-1"></a><a class="docs-heading-anchor-permalink" href="#Software-Interface" title="Permalink"></a></h2><p><code>DistStat.jl</code> implements a distributed <code>MPI</code>-based array data structure <code>MPIArray</code> as the core data structure for implementations of <code>AbstractArray</code>s. It uses <a href="https://github.com/JuliaParallel/MPI.jl"><code>MPI.jl</code></a> as a backend. It has been tested for basic <code>Array</code>s and <code>CuArray</code>s from <a href="https://github.com/JuliaGPU/CUDA.jl"><code>CUDA.jl</code></a>. The standard vectorized &quot;dot&quot; operations can be used for convenient element-by-element operations as well as broadcasting operations on <code>MPIArray</code>s. Furthermore, simple distributed matrix operations for <code>MPIMatrix</code>, or two-dimensional <code>MPIArray</code>s, are also implemented. Reduction and accumulation operations are supported for <code>MPIArrays</code> of any dimensions.   The package can be loaded by:</p><pre><code class="language-julia">using DistStat</code></pre><p>If GPUs are available, one that is to be used is automatically selected in a round-robin fashion upon loading the package. The rank, or the &quot;ID&quot; of a process, and the size, or the total number of the processes, can be accessed by:</p><pre><code class="language-julia">DistStat.Rank()
DistStat.Size()</code></pre><p>Ranks are indexed 0-based, following the <code>MPI</code> standard.</p><h3 id="Data-Structure-for-Distributed-MPI-Array"><a class="docs-heading-anchor" href="#Data-Structure-for-Distributed-MPI-Array">Data Structure for Distributed MPI Array</a><a id="Data-Structure-for-Distributed-MPI-Array-1"></a><a class="docs-heading-anchor-permalink" href="#Data-Structure-for-Distributed-MPI-Array" title="Permalink"></a></h3><p>In <code>DistStat.jl</code>, a distributed array data type <code>MPIArray{T,N,AT}</code> is defined. Here, parameter <code>T</code> is the type of each element of an array, e.g., <code>Float64</code> or <code>Float32</code>. Parameter <code>N</code> is the dimension of the array, <code>1</code> for vector and <code>2</code> for matrix. Parameter <code>AT</code> is the implementation of <code>AbstractArray</code> used for base operations: <code>Array</code> for CPU array, and <code>CuArray</code> for the arrays on Nvidia GPUs (requires <code>CUDA.jl</code>). If there are multiple CUDA devices, a device is assigned to a process automatically by teh rank of the process modulo the size. This assignment scheme extends to the setting in which there are multiple GPU devices in multiple CPU nodes. The type <code>MPIArray{T,N,AT}</code> is a subtype of <code>AbstractArray{T,N}</code>. In <code>MPIArray{T,N,AT</code>, each rank holds a contiguous block of the full data in <code>AT{T,N}</code> split by the <code>N</code>-th dimension, or the last dimension of an <code>MPIArray</code>.</p><p>In the special case of a two-dimenaional array, aliased by <code>MPIMatrix{T,AT}</code>, the data is column-major ordered and column-split. The transpose of this matrix has type of  <code>Transpose{T,MPIMatrix{T,AT}}</code> which is row-major ordered, row-split. There also is an alias for one-dimensional array <code>MPIArray{T,1,A}</code>, which is <code>MPIVector{T,A}</code>.</p><h4 id="Creation"><a class="docs-heading-anchor" href="#Creation">Creation</a><a id="Creation-1"></a><a class="docs-heading-anchor-permalink" href="#Creation" title="Permalink"></a></h4><p>The syntax <code>MPIArray{T,N,A}(undef, m, ...)</code> creates an ininitialized <code>MPIArray</code>. For example,</p><pre><code class="language-julia">a = MPIArray{Float64, 2, Array}(undef, 3, 4)</code></pre><p>creates an uninitialized 3 <span>$\times$</span> 4 distributed array based on local <code>Array</code>s of double precision floating-point numbers. The size of this array, the type of each element, and number of dimensions can be accessed using the usual functions in Julia:</p><pre><code class="language-julia">size(a)
eltype(a)
ndims(a)</code></pre><p>Local data held by each process can be accessed by appending <code>.localarray</code> to the name of the array, e.g., </p><pre><code class="language-julia">a.localarray</code></pre><p>Matrices are split as evenly as possible. For example, if the number of processes is 4 and the <code>size(a) == (3, 7)</code>, processes of rank 0 through 2 hold the local data of size (3, 2) and the rank-3 process holds the local data of size (3, 1).</p><p>An <code>MPIArray</code> can also be created by distributing an array in a single process. For example, in the following code: </p><pre><code class="language-julia">if DistStat.Rank() == 0
    dat = [1, 2, 3, 4]
else
    dat = Array{Int64}(undef, 0)
end
d = distribute(dat)</code></pre><p>the data is defined in rank 0 process, and the other processes have an empty instance of <code>Array{Int64}</code>. Using the function <code>distribute</code>, the <code>MPIArray{Int64, 1, Array}</code> of the data <code>[1, 2, 3, 4]</code>, equally distributed over four processes, is created.</p><h4 id="Filling-an-array"><a class="docs-heading-anchor" href="#Filling-an-array">Filling an array</a><a id="Filling-an-array-1"></a><a class="docs-heading-anchor-permalink" href="#Filling-an-array" title="Permalink"></a></h4><p>An <code>MPIArray</code> <code>a</code> can be filled with a  number <code>x</code> using the usual syntax of the function <code>fill!(a, x)</code>. For example, <code>a</code> can be filled with zero:</p><pre><code class="language-julia">fill!(a, 0)</code></pre><h4 id="Random-number-generation"><a class="docs-heading-anchor" href="#Random-number-generation">Random number generation</a><a id="Random-number-generation-1"></a><a class="docs-heading-anchor-permalink" href="#Random-number-generation" title="Permalink"></a></h4><p>An array can also be filled with random values, extending <code>Random.rand!()</code> for the standard uniform distribution and <code>Random.randn()!</code> for the standard normal distribution. The following code fills <code>a</code> with uniform(0, 1) random numbers:</p><pre><code class="language-julia">using Random
rand!(a)</code></pre><p>In cases such as unit testing, generating identical data for any configuration is important. For this purpose, the following interface is defined:</p><pre><code class="language-julia">function rand!(a::MPIArray{T,N,A}; seed=nothing, common_init=false, root=0) where {T,N,A}</code></pre><p>If the keyword argument <code>common_init=true</code> is set, the data are generated from the process with rank <code>root</code>. The <code>seed</code> can also be configured. If <code>commonn_init==false</code> and <code>seed==k</code>, the seed for each process is defined by <code>k</code> plus the rank.</p><h3 id="&quot;Dot&quot;-syntax-and-vectorization"><a class="docs-heading-anchor" href="#&quot;Dot&quot;-syntax-and-vectorization">&quot;Dot&quot; syntax and vectorization</a><a id="&quot;Dot&quot;-syntax-and-vectorization-1"></a><a class="docs-heading-anchor-permalink" href="#&quot;Dot&quot;-syntax-and-vectorization" title="Permalink"></a></h3><p>The &quot;dot&quot; broadcasting feature of <code>DistStat.jl</code> follows the standard Julia syntax. This syntax provides a convenient way to operate on both multi-node clusters and multi-GPU workstations with the same code. For example, the soft-thresholding operator, which commonly appears in sparse regression can be defined in the element level:</p><pre><code class="language-julia">function soft_threshold(x::T, λ::T) ::T where T &lt;: AbstractFloat
    x &gt; λ &amp;&amp; return (x - λ)
    x &lt; -λ &amp;&amp; return (x + λ)
    return zero(T)
end</code></pre><p>This function can be applied to each element of an <code>MPIArray</code> using the dot broadcasting, as follows. When the dot operation is used for an <code>MPIArray{T,N,AT}</code>, it is naturally passed to inner array implementation <code>AT</code>. Consider the following arrays filled with random numbers from the standard normal distribution:</p><pre><code class="language-julia">a = MPIArray{Float64, 2, Array}(undef, 2, 4) |&gt; randn!
b = MPIArray{Float64, 2, Array}(undef, 2, 4) |&gt; randn!</code></pre><p>The function <code>soft_threshold()</code> is applied elementwisely as the following:</p><pre><code class="language-julia">a .= soft_threshold.(a .+ 2 .* b, 0.5)</code></pre><p>The three dot operations, <code>.=</code>, <code>.+</code>, and <code>.*</code>, are fused into a single loop (in CPU) or a single kernel (in GPU) internally. </p><p>A singleton non-last dimension is treated as if the array is repeated along that dimension, just like <code>Array</code> operations.</p><pre><code class="language-julia">c = MPIArray{Float64, 2, Array}(undef, 1, 4) |&gt; rand!
a .= soft_threshold.(a .+ 2 .* c, 0.5)</code></pre><p>works as if <code>c</code> were a <span>$2 \times 4$</span> array, with its content repeated twice.</p><p>It is a little bit subtle with the last dimension, as the <code>MPIArray{T,N,AT}</code>s are split along that dimension. It works if the broadcast array has the type <code>AT</code> and holds the same data across the processes. For example,</p><pre><code class="language-julia">d = Array{Float64}(undef, 2, 1); fill!(d, -0.1)
a .= soft_threshold.(a .+ 2 .* d, 0.5)</code></pre><p>As with any dot operations in Julia, the operatons for <code>DistStat.jl</code> are convenient but usually not the fastest option, Its implementations can be further optimized by specializeing in specific array types.</p><h3 id="Reduction-opeartions-and-accumulation-operations"><a class="docs-heading-anchor" href="#Reduction-opeartions-and-accumulation-operations">Reduction opeartions and accumulation operations</a><a id="Reduction-opeartions-and-accumulation-operations-1"></a><a class="docs-heading-anchor-permalink" href="#Reduction-opeartions-and-accumulation-operations" title="Permalink"></a></h3><p>Reduction operations, such as <code>sum()</code>, <code>prod()</code>, <code>maximum()</code>, <code>minimum()</code>, and accumulations such as <code>cumsum()</code>, <code>cumsum!()</code>, <code>cumprod()</code>, <code>cumprod!()</code> are implemented just like their base counterparts, computing cumulative sums and products. Example usages of <code>sum()</code> and <code>sum!()</code> are:</p><pre><code class="language-julia">sum(a)
sum(abs2, a) # sum of squared absolute value
sum(a, dims=1) # column sums
sum(a, dims=2)
sum(a, dims=(1,2)) # returns 1x1 MPIArray
sum!(c, a) # columnwise sum 
sum!(d, a) # rowwise sum</code></pre><p>The first line computes the elementwise sum of <code>a</code>. The second line computes the sum of squared absolute values (<code>abs2()</code> is the method that computes the squared absolute values). The third and fourth lines compute the column sums and row sums, respectively. Similar to the dot operations, the third line reduces along the distributed dimensions, and returns a broadcast local <code>Array</code>. The fifth line returns the sum of all elements, but the data type is a <span>$1 \times 1$</span> <code>MPIArray</code>. The syntax <code>sum!(p, q)</code> selects which dimension to reduce based on the shape of <code>p</code>, the first argument. The sixth line computes the columnwise sum and saves it to <code>c</code>, because <code>c</code> is a <span>$1 \times 4$</span> <code>MPIArray</code>. The seventh line computes rowwise sum, because <code>d</code> is a <span>$2 \times 1$</span> local <code>Array</code>.  </p><p>Given below are examples for <code>cumsum()</code> and <code>cumsum!()</code>:</p><pre><code class="language-julia"># Accumulations
cumsum(a; dims=1) # columnwise accumulation
cumsum(a; dims=2) # rowwise accumulation
cumsum!(b, a; dims=1) # columnwise, result saved in `b`
cumsum!(b, a; dims=2)</code></pre><p>The first line computes the columnwise cumulative sum, and the second line computes the rowwise cumulative sum. So do the third and fourth lines, but save the results in <code>b</code>, which has the same size as <code>a</code>. </p><h3 id="Distributed-Linear-Algebra"><a class="docs-heading-anchor" href="#Distributed-Linear-Algebra">Distributed Linear Algebra</a><a id="Distributed-Linear-Algebra-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-Linear-Algebra" title="Permalink"></a></h3><h4 id="Dot-product"><a class="docs-heading-anchor" href="#Dot-product">Dot product</a><a id="Dot-product-1"></a><a class="docs-heading-anchor-permalink" href="#Dot-product" title="Permalink"></a></h4><p>The method <code>LinearAlgebra.dot()</code> for <code>MPIArray</code>s is defined just like the base <code>LinearAlgebra.dot()</code>, which sums all the elements after an elementwise multiplication of the two argument arrays: </p><pre><code class="language-julia">using LinearAlgebra
dot(a, b)</code></pre><h4 id="Operations-on-the-diagonals"><a class="docs-heading-anchor" href="#Operations-on-the-diagonals">Operations on the diagonals</a><a id="Operations-on-the-diagonals-1"></a><a class="docs-heading-anchor-permalink" href="#Operations-on-the-diagonals" title="Permalink"></a></h4><p>The &quot;getter&quot; method for the diagonal, <code>diag!(d, a)</code>, and the &quot;setter&quot; method for the diagonal, <code>fill_diag!()</code>, are also available. The former obtains the main diagonal of the <code>MPIMatrix</code> <code>a</code> and is stored in <code>d</code>. If <code>d</code> is an <code>MPIMatrix</code> with a single row, the result is obtained in a distributed form. On the other hand, if <code>d</code> is a local <code>AbstractArray</code>, all elements of the main diagonal is copied to all processes as a broadcast <code>AbstractArray</code>:</p><pre><code class="language-julia">M = MPIMatrix{Float64, Array}(undef, 4, 4) |&gt; rand!
v_dist = MPIMatrix{Float64, Array}(undef, 1, 4)
v = Array{Float64}(undef, 4)
diag!(v_dist, M)
diag!(v, M)</code></pre><h4 id="Matrix-multiplication"><a class="docs-heading-anchor" href="#Matrix-multiplication">Matrix multiplication</a><a id="Matrix-multiplication-1"></a><a class="docs-heading-anchor-permalink" href="#Matrix-multiplication" title="Permalink"></a></h4><p>The method <code>LinearAlgebra.mul!(C, A, B)</code> is implemented for <code>MPIMatrix</code>, in which the multiplication of <code>A</code> and <code>B</code> is stored in <code>C</code>. Matrix multiplications for 17 different combinations of types for <code>A</code>, <code>B</code>, and <code>C</code>, including matrix-vector multiplications are included in the package. It is worth noting that transpose of an <code>MPIMatrix</code> is a row-major ordered, row-split matrix. While the base syntax of <code>mul!(C, A, B)</code> is always available, any temporary memory to save intermediate results can also be provided as a keyword argument in order to avoid repetitive allocations in iterative algorithms, as in <code>mul!(C, A, B; tmp=Array(undef, 3, 4)</code>. The user should determine which shape of <code>C</code> minimizes communication and suits better for their application. <code>MPIColVector{T, AT}</code> is defined as <code>Union{MPIVector{T,AT}, Transpose{T, MPIMatrix{T,AT}}}</code> to include transposed <code>MPIMatrix</code> with a single row. The 17 possible combinations of arguments available are listed below:</p><pre><code class="language-julia">LinearAlgebra.mul!(C::MPIMatrix{T,AT}, A::MPIMatrix{T,AT}, B::Transpose{T, MPIMatrix{T,AT}};
                            tmp::AbstractArray{T,2}=AT{T}(undef, size(A,1), size(B,2))) where {T,AT}
LinearAlgebra.mul!(C::Transpose{T,MPIMatrix{T,AT}}, A::MPIMatrix{T,AT},B::Transpose{T,MPIMatrix{T,AT}};
                            tmp::AbstractArray{T,2}=AT{T}(undef, size(B,2), size(A,1))) where {T,AT}
LinearAlgebra.mul!(C::MPIMatrix{T,AT}, A::MPIMatrix{T,AT}, B::MPIMatrix{T,AT};
                            tmp::AbstractArray{T,2}=AT{T}(undef, size(A,1), size(A,2))) where {T,AT}
LinearAlgebra.mul!(C::Transpose{T,MPIMatrix{T,AT}}, A::Transpose{T,MPIMatrix{T,AT}}, B::Transpose{T,MPIMatrix{T,AT}};
                            tmp::AbstractArray{T,2}=AT{T}(undef, size(B,2), size(B,1))) where {T,AT}
LinearAlgebra.mul!(C::AbstractMatrix{T}, A::MPIMatrix{T,AT}, B::Transpose{T,MPIMatrix{T,AT}}) where {T,AT}
LinearAlgebra.mul!(C::MPIMatrix{T,AT}, A::Transpose{T,MPIMatrix{T,AT}}, B::MPIMatrix{T,AT};
                            tmp::AbstractArray{T,2}=AT{T}(undef,size(A,2),size(A,1))) where {T,AT}
LinearAlgebra.mul!(C::Transpose{T,MPIMatrix{T,AT}}, A::Transpose{T,MPIMatrix{T,AT}}, B::MPIMatrix{T,AT};
                            tmp::AbstractArray{T,2}=AT{T}(undef,size(B,1), size(B,2))) where {T,AT}
LinearAlgebra.mul!(C::MPIMatrix{T,AT}, A::Union{AbstractMatrix{T}}, B::MPIMatrix{T,AT}) where {T,AT}
LinearAlgebra.mul!(C::MPIMatrix{T,AT}, A::Transpose{T,ATT} where ATT &lt;: AbstractMatrix{T}, B::MPIMatrix{T,AT}) where {T,AT}
LinearAlgebra.mul!(C::Transpose{T,MPIMatrix{T,AT}}, A::Transpose{T,MPIMatrix{T,AT}},
                            B::Transpose{T, ATT} where ATT &lt;: AbstractMatrix{T}) where {T,AT}
LinearAlgebra.mul!(C::Transpose{T,MPIMatrix{T,AT}}, A::Transpose{T,MPIMatrix{T,AT}}, B::AbstractMatrix{T}) where {T,AT}
LinearAlgebra.mul!(C::AbstractVector{T}, A::MPIMatrix{T,AT}, B::AbstractVector{T}) where {T,AT}
LinearAlgebra.mul!(C::AbstractVector{T}, A::Transpose{T, MPIMatrix{T,AT}}, B::AbstractVector{T}) where {T,AT}
const MPIColVector{T,AT} = Union{MPIVector{T,AT},Transpose{T,MPIMatrix{T,AT}}}
LinearAlgebra.mul!(C::AbstractVector{T}, A::MPIMatrix{T,AT}, B::MPIColVector{T,AT}) where {T,AT}
LinearAlgebra.mul!(C::MPIColVector{T,AT}, A::Transpose{T,MPIMatrix{T,AT}}, B::AbstractVector{T}) where {T,AT} 
LinearAlgebra.mul!(C::MPIColVector{T,AT}, A::MPIMatrix{T,AT}, B::MPIColVector{T,AT};
                            tmp::AbstractArray{T}=AT{T}(undef, size(C, 1))) where {T,AT}
LinearAlgebra.mul!(C::MPIColVector{T,AT}, A::Transpose{T,MPIMatrix{T,AT}},B::MPIColVector{T,AT};
                            tmp::AbstractArray{T}=AT{T}(undef, size(B,1))) where {T,AT}</code></pre><h4 id="Operator-norms"><a class="docs-heading-anchor" href="#Operator-norms">Operator norms</a><a id="Operator-norms-1"></a><a class="docs-heading-anchor-permalink" href="#Operator-norms" title="Permalink"></a></h4><p>The method <code>opnorm()</code> either evaluates (<span>$\ell_1$</span> and <span>$\ell_\infty$</span>) or approximates (<span>$\ell_2$</span>)  matrix operator norms, defined for a matrix <span>$A \in \mathbb{R}^{m \times n}$</span> as <span>$\|A\| = \sup\{\|Ax\|: x \in \mathbb{R}^n \text{ with } \|x\| = 1\}$</span> for each respective vector norm. </p><pre><code class="language-julia">opnorm(a, 1)
opnorm(a, 2)
opnorm(a, Inf)</code></pre><p>The <span>$\ell_2$</span>-norm estimation implements the power iteration, and can be further configured for convergence and number of iterations. There also is an implementation based on the inequality <span>$\|A\|_2 \le \|A\|_1 \|A\|_\infty$</span> (<code>method=&quot;quick&quot;</code>), which overestimates the <span>$\ell_2$</span>-norm.</p><pre><code class="language-julia">opnorm(a, 2; method=&quot;power&quot;, tol=1e-6, maxiter=1000, seed=95376)</code></pre><pre><code class="language-julia">opnorm(a, 2; method=&quot;quick&quot;)</code></pre><h3 id="Simplified-MPI-Interfaces"><a class="docs-heading-anchor" href="#Simplified-MPI-Interfaces">Simplified MPI Interfaces</a><a id="Simplified-MPI-Interfaces-1"></a><a class="docs-heading-anchor-permalink" href="#Simplified-MPI-Interfaces" title="Permalink"></a></h3><p><code>DistStat.jl</code> also provides a simplified version of <code>MPI</code> primitives. These primitives allow omission of the constant <code>MPI.COMM_WORLD</code>, tag for communication, and the root if it is zero. They are modeled after the convention of the <code>distributed</code> subpackage of <code>PyTorch</code>, but with richer interfaces including <code>Allgatherv!()</code>, <code>Gatherv!()</code>, and <code>Scatterv!()</code>. These primitives are not only exposed to the user, but also extensively used in the linear algebra and array operation routines explained above. <code>Array</code>s can be used as arguments fo rthe following functions, and <code>CuArrays</code> can be used if <code>CUDA</code>-aware <code>MPI</code> implementation such as <code>OpenMPI</code> is available on the system. The following list shows the signatures of simplified <code>MPI</code> methods. See <a href="https://juliaparallel.github.io/MPI.jl/latest/">documentations</a> of <code>MPI.jl</code> for more information.</p><pre><code class="language-julia">Barrier()
Bcast!(arr::AbstractArray; root::Integer=0)
Send(arr::AbstractArray, dest::Integer; tag::Integer=0)
Recv!(arr::AbstractArray, src::Integer; tag::Integer=0)
Isend(arr::AbstractArray, dest::Integer; tag::Integer=0)
Irecv!(arr::AbstractArray, src::Integer; tag::Integer=0)
Reduce!(sendarr::AbstractArray, recvarr::AbstractArray; 
    op=MPI.SUM, root::Integer=0)
Allreduce!(arr::AbstractArray; op=MPI.SUM)
Allgather!(sendarr::AbstractArray, recvarr::AbstractArray)
Allgather!(sendarr::AbstractArray, recvarr::AbstractArray, 
    count::Integer)
Allgatherv!(sendarr::AbstractArray, recvarr::AbstractArray, 
    counts::Vector{&lt;:Integer})
Scatter!(sendarr::AbstractArray, recvarr::AbstractArray; 
    root::Integer=0)
Scatterv!(sendarr::AbstractArray, recvarr::AbstractArray, 
    counts::Vector{&lt;:Integer}; root::Integer=0)
Gather!(sendarr::AbstractArray, recvarr::AbstractArray; 
    root::Integer=0)
Gatherv!(sendarr::AbstractArray, recvarr::AbstractArray, 
    counts::Vector{&lt;:Integer}; root::Integer=0)</code></pre></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 28 October 2020 13:31">Wednesday 28 October 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
